
## Notebook Link

You can access the project notebook here:

[Image Captioning with ResNet50 and Transformers - Colab Notebook](https://colab.research.google.com/drive/1qAEb808UpqdVRFxSXhvngXka67qKfuHz?usp=sharing)


# Image Captioning with Deep Learning

This project implements an image captioning system using a combination of Convolutional Neural Networks (CNN) for image feature extraction and Recurrent Neural Networks (RNN), specifically LSTM, for generating captions. We utilize the Flickr8K dataset for training and testing the model.

## Steps Overview

### 1. **Data Setup**

The first step involves downloading and preparing the dataset:

- **Flickr8K Dataset**: The images are stored in a directory, and captions are provided for each image in a separate file.
- **Dataset Paths**: The dataset is organized into `Images` for the images and a `captions.txt` file containing the corresponding captions.

### 2. **Feature Extraction**

We use a pre-trained **ResNet50** model to extract features from each image. These features represent the high-level information captured from each image, which is then fed into the model for caption generation.

- **Pre-trained Model**: ResNet50 is used as a feature extractor, excluding the top classification layers.
- **Image Preprocessing**: Images are resized to `224x224` and preprocessed to match the input format required by ResNet50.
- **Feature Dictionary**: A dictionary (`id_to_features`) is created, where the key is the image ID, and the value is the extracted feature vector.

### 3. **Caption Preprocessing**

The captions corresponding to each image are cleaned and tokenized:

- **Cleaning**: Captions are converted to lowercase, and special characters are removed.
- **Tokenization**: A tokenizer is used to convert words into sequences of integers. Additionally, the captions are wrapped with `<start>` and `<end>` tokens to indicate the beginning and end of each caption.
- **Padding**: The caption sequences are padded to ensure uniform length.

### 4. **Model Architecture**

A custom architecture is used for generating captions. The model consists of two main parts:

- **Image Feature Processing**: The ResNet50 features are passed through a few dense layers to process the image information.
- **Caption Sequence Processing**: The caption sequence is processed using an LSTM network to predict the next word based on the previous words and the image features.
  
The model is compiled with **categorical cross-entropy** loss and **Adam** optimizer.

### 5. **Data Generator**

A data generator function is implemented to yield batches of image features and caption sequences for training. This generator is particularly useful for handling large datasets, as it loads data in batches during training.

- **Generator Inputs**: The generator takes the image IDs and corresponding captions, processes the sequences, and feeds them along with the image features to the model.
  
### 6. **Model Training**

The model is trained for 20 epochs using the prepared dataset. The training process uses the `data_generator` to provide batches of data to the model, which learns to predict the next word in the caption given an image's feature vector and previous words in the caption.

- **Epochs**: 20
- **Batch Size**: 16
- **Train/Test Split**: The dataset is split into 90% training and 10% testing.

### 7. **Prediction and Evaluation**

After training the model, it is evaluated by generating captions for random test images. The predicted captions are compared to the actual captions using **cosine similarity** to measure how close the predicted caption is to the true caption.

- **Cosine Similarity**: The similarity between the predicted and true captions is computed based on their embeddings.

### 8. **Real-Time Implementation**

In the final part, the model is used to generate captions for random test images in real-time. The images and their predicted captions are displayed.

- **Visualization**: Random images are selected from the test set, and captions generated by the model are displayed alongside the images.
  
---

## Requirements

- Python 3.x
- TensorFlow 2.x
- Keras
- Numpy
- TQDM
- scikit-learn
- PIL
- KaggleHub (for dataset download)



## Usage

1. **Dataset Download**: The dataset can be downloaded using KaggleHub by running the command:

   ```python
   path = kagglehub.dataset_download('adityajn105/flickr8k')
   ```

2. **Model Training**: Once the data is prepared, the model can be trained using the data generator:

   ```python
   model = RecursiveModel(max_sequence_length, vocab_size)
   model.train(data_generator)
   ```

3. **Prediction**: After training, the model can generate captions for any image in the dataset:

   ```python
   predicted_caption = model.predict(image_features)
   ```

4. **Real-Time Captioning**: For generating captions for random images:

   ```python
   predicted_caption = predict_caption(model, image_features, tokenizer, max_sequence_length)
   ```





